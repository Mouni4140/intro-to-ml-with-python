{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Collecting Data from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:32:57.440181Z",
     "start_time": "2019-12-15T05:32:54.954408Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:45:57.554807Z",
     "start_time": "2019-12-15T05:45:55.844514Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:14:36.500842Z",
     "start_time": "2019-12-15T06:14:34.461455Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:19:23.899147Z",
     "start_time": "2019-12-15T06:19:22.442006Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:11:28.597641Z",
     "start_time": "2019-12-15T06:11:28.581110Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:39:31.158732Z",
     "start_time": "2019-12-15T05:39:29.865946Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "path=\"data/reuters_data/\"\n",
    "files=os.listdir(path)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:44:46.366412Z",
     "start_time": "2019-12-15T05:44:46.362526Z"
    }
   },
   "outputs": [],
   "source": [
    "f=open(os.path.join(path, files[1]),'r',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:44:46.738560Z",
     "start_time": "2019-12-15T05:44:46.733033Z"
    }
   },
   "outputs": [],
   "source": [
    "text=\"\"\n",
    "for line in f:\n",
    "    if line.strip()==\"\":continue #if line is blank skip it\n",
    "    else:\n",
    "        text+=' '+line.strip() #else append all lines\n",
    "print(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:45:12.990320Z",
     "start_time": "2019-12-15T05:45:12.913010Z"
    }
   },
   "outputs": [],
   "source": [
    "target=[]\n",
    "article_text=[]\n",
    "for file in files:\n",
    "    if '.txt' not in file:continue\n",
    "    f=open(os.path.join(path,file),\n",
    "           encoding='latin-1')\n",
    "    article_text.append(\" \".join([line.strip() for line in\n",
    "                                       f if line.strip()!=\"\"]))\n",
    "    if \"crude\" in file:\n",
    "        target.append(\"crude\")\n",
    "    else:\n",
    "        target.append(\"money\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:45:25.799516Z",
     "start_time": "2019-12-15T05:45:25.790840Z"
    }
   },
   "outputs": [],
   "source": [
    "mydata=pd.DataFrame({'target':target,'article_text':article_text})\n",
    "mydata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:45:14.426222Z",
     "start_time": "2019-12-15T05:45:14.402865Z"
    }
   },
   "outputs": [],
   "source": [
    "mydata.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Text Data Exploration with word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Text data visualisation. Words with high frequency appear larger and words with low frequency appear smaller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:46:07.433921Z",
     "start_time": "2019-12-15T05:46:07.373903Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:46:08.593734Z",
     "start_time": "2019-12-15T05:46:08.412222Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:46:25.044883Z",
     "start_time": "2019-12-15T05:46:25.032973Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_articles=' '.join(mydata['article_text']) #all articles combined into one single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:46:25.868593Z",
     "start_time": "2019-12-15T05:46:25.857165Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "crude_articles=\" \".join(mydata.loc[mydata['target']=='crude',\n",
    "                                   'article_text']) #all crude articles combined into one single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:46:26.521021Z",
     "start_time": "2019-12-15T05:46:26.516105Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "money_articles=\" \".join(mydata.loc[mydata['target']=='money',\n",
    "                                   'article_text']) #all money articles combined into one single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:46:31.579947Z",
     "start_time": "2019-12-15T05:46:30.391242Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(all_articles)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:46:35.588093Z",
     "start_time": "2019-12-15T05:46:34.729553Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(crude_articles)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:46:44.443395Z",
     "start_time": "2019-12-15T05:46:43.614388Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(money_articles)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Text Data Exploration with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:52:09.271858Z",
     "start_time": "2019-12-15T05:52:09.268796Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:52:47.396902Z",
     "start_time": "2019-12-15T05:52:46.695850Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokens=word_tokenize(money_articles) #Breaking down the text into individual words\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:53:48.900046Z",
     "start_time": "2019-12-15T05:53:48.893952Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "money_articles_Text=nltk.Text(tokens) #nltk based data structure\n",
    "money_articles_Text, type(money_articles_Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:54:45.298385Z",
     "start_time": "2019-12-15T05:54:45.220388Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "money_articles_Text.concordance('dollar') #for understanding the context of given words, as in where they are used in to article\n",
    "#money_articles_Text.concordance('dlr')\n",
    "#money_articles_Text.concordance('bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:54:50.026214Z",
     "start_time": "2019-12-15T05:54:49.253878Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "money_articles_Text.similar('bank') #this is derived based on common contexts. Not as efficient as human reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "money_articles_Text.common_contexts(['dollar','dlr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Text Data features with tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:05:50.582667Z",
     "start_time": "2019-12-15T06:05:50.579662Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:05:51.090416Z",
     "start_time": "2019-12-15T06:05:51.079096Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemma = WordNetLemmatizer() #for combining similar words together\n",
    "\n",
    "my_stop=set(stopwords.words('english')+list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:06:17.692108Z",
     "start_time": "2019-12-15T06:06:17.688290Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lemma.lemmatize(\"peoples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:07:09.448648Z",
     "start_time": "2019-12-15T06:07:09.442011Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "my_stop #these words will not be used for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:09:42.750252Z",
     "start_time": "2019-12-15T06:09:42.745577Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split_into_lemmas(message):\n",
    "    message=message.lower()\n",
    "    words = word_tokenize(message)\n",
    "    words_sans_stop=[]\n",
    "    for word in words :\n",
    "        if word in my_stop:continue\n",
    "        words_sans_stop.append(word)\n",
    "    return [lemma.lemmatize(word) for word in words_sans_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:09:43.084865Z",
     "start_time": "2019-12-15T06:09:43.081184Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfidf= TfidfVectorizer(analyzer=split_into_lemmas,\n",
    "                       min_df=20,\n",
    "                       max_df=500,\n",
    "                       stop_words=my_stop)\n",
    "#min no of occurence for a word to be considered for analysis is min_df = 50\n",
    "#max_df is said to ignore the very common words that are redundant for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:10:30.402761Z",
     "start_time": "2019-12-15T06:10:28.452529Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfidf.fit(mydata['article_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:10:33.696397Z",
     "start_time": "2019-12-15T06:10:31.702358Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfidf_data=tfidf.transform(mydata['article_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:10:34.556747Z",
     "start_time": "2019-12-15T06:10:34.551783Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfidf_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:10:37.107401Z",
     "start_time": "2019-12-15T06:10:37.103302Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfidf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sentiment Analysis with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:11:18.466829Z",
     "start_time": "2019-12-15T06:11:18.440704Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer=SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:14:16.629685Z",
     "start_time": "2019-12-15T06:14:16.625281Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = [\"VADER is smart, handsome, and funny.\",      # positive sentence example\n",
    "            \"VADER is not smart, handsome, nor funny.\",   # negation sentence example\n",
    "            \"VADER is smart, handsome, and funny!\",       # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "            \"VADER is very smart, handsome, and funny.\",  # booster words handled correctly (sentiment intensity adjusted)\n",
    "            \"VADER is VERY SMART, handsome, and FUNNY.\",  # emphasis for ALLCAPS handled\n",
    "            \"VADER is VERY SMART, handsome, and FUNNY!!!\",# combination of signals - VADER appropriately adjusts intensity\n",
    "            \"VADER is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!\",# booster words & punctuation make this close to ceiling for score\n",
    "            \"The book was good.\",                                     # positive sentence\n",
    "            \"The book was kind of good.\",                 # qualified positive sentence is handled correctly (intensity adjusted)\n",
    "        '''The plot was good, but the characters are uncompelling and \n",
    "             the dialog is not great.''', # mixed negation sentence\n",
    "            \"At least it isn't a horrible book.\",         # negated negative sentence with contraction\n",
    "            \"Make sure you :) or :D today!\",              # emoticons handled\n",
    "            \"Today SUX!\",                                 # negative slang with capitalization emphasis\n",
    "            \"Today only kinda sux! But I'll get by, lol\"  # mixed sentiment example with slang and constrastive conjunction \"but\"\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:14:17.038345Z",
     "start_time": "2019-12-15T06:14:17.027645Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    print(sentence, str(vs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Language Correction, Detection and Translation with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:15:28.692888Z",
     "start_time": "2019-12-15T06:15:28.636767Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:16:26.502907Z",
     "start_time": "2019-12-15T06:16:26.323057Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "b = TextBlob(\"I havv goood color !\")\n",
    "b.correct(), b.detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:16:27.006936Z",
     "start_time": "2019-12-15T06:16:27.003916Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "chinese_blob = TextBlob(u\"美丽优于丑陋\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:16:27.847890Z",
     "start_time": "2019-12-15T06:16:27.751790Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "chinese_blob.translate(from_lang=\"zh-CN\", to='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:16:28.544742Z",
     "start_time": "2019-12-15T06:16:28.541587Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "b = TextBlob(u\"بسيط هو أفضل من مجمع\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:16:29.233902Z",
     "start_time": "2019-12-15T06:16:29.133010Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "b.detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source : https://stackoverflow.com/questions/1833252/java-stanford-nlp-part-of-speech-labels\n",
    "\n",
    "* CC: conjunction, coordinating\n",
    "\n",
    "    & 'n and both but either et for less minus neither nor or plus so\n",
    "    therefore times v. versus vs. whether yet\n",
    "    \n",
    "* CD: numeral, cardinal\n",
    "\n",
    "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
    "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
    "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
    "    \n",
    "* DT: determiner\n",
    "\n",
    "    all an another any both del each either every half la many much nary\n",
    "    neither no some such that the them these this those\n",
    "    \n",
    "* EX: existential there\n",
    "    there\n",
    "    \n",
    "* FW: foreign word\n",
    "\n",
    "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
    "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
    "    terram fiche oui corporis ...\n",
    "    \n",
    "* IN: preposition or conjunction, subordinating\n",
    "\n",
    "    astride among uppon whether out inside pro despite on by throughout\n",
    "    below within for towards near behind atop around if like until below\n",
    "    next into if beside ...\n",
    "    \n",
    "* JJ: adjective or numeral, ordinal\n",
    "\n",
    "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
    "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
    "    multilingual multi-disciplinary ...\n",
    "    \n",
    "* JJR: adjective, comparative\n",
    "\n",
    "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
    "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
    "    cozier creamier crunchier cuter ...\n",
    "    \n",
    "* JJS: adjective, superlative\n",
    "\n",
    "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
    "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
    "    dearest deepest densest dinkiest ...\n",
    "    \n",
    "* LS: list item marker\n",
    "\n",
    "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
    "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
    "    two\n",
    "    \n",
    "* MD: modal auxiliary\n",
    "\n",
    "    can cannot could couldn't dare may might must need ought shall should\n",
    "    shouldn't will would\n",
    "    \n",
    "* NN: noun, common, singular or mass\n",
    "\n",
    "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
    "    investment slide humour falloff slick wind hyena override subhumanity\n",
    "    machinist ...\n",
    "    \n",
    "* NNS: noun, common, plural\n",
    "\n",
    "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
    "    divestitures storehouses designs clubs fragrances averages\n",
    "    subjectivists apprehensions muses factory-jobs ...\n",
    "    \n",
    "* NNP: noun, proper, singular\n",
    "\n",
    "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
    "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
    "    Shannon A.K.C. Meltex Liverpool ...\n",
    "    \n",
    "* NNPS: noun, proper, plural\n",
    "\n",
    "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
    "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
    "    Apache Apaches Apocrypha ...\n",
    "    \n",
    "* PDT: pre-determiner\n",
    "\n",
    "    all both half many quite such sure this\n",
    "    \n",
    "* POS: genitive marker\n",
    "    ' 's\n",
    "\n",
    "* PRP: pronoun, personal\n",
    "\n",
    "    hers herself him himself hisself it itself me myself one oneself ours\n",
    "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
    "    \n",
    "* PRP$': pronoun, possessive\n",
    "\n",
    "    her his mine my our ours their thy your\n",
    "    \n",
    "* RB: adverb\n",
    "\n",
    "    occasionally unabatingly maddeningly adventurously professedly\n",
    "    stirringly prominently technologically magisterially predominately\n",
    "    swiftly fiscally pitilessly ...\n",
    "    \n",
    "* RBR: adverb, comparative\n",
    "\n",
    "    further gloomier grander graver greater grimmer harder harsher\n",
    "    healthier heavier higher however larger later leaner lengthier less-\n",
    "    perfectly lesser lonelier longer louder lower more ...\n",
    "    \n",
    "* RBS: adverb, superlative\n",
    "\n",
    "    best biggest bluntest earliest farthest first furthest hardest\n",
    "    heartiest highest largest least less most nearest second tightest worst\n",
    "    \n",
    "* RP: particle\n",
    "\n",
    "    aboard about across along apart around aside at away back before behind\n",
    "    by crop down ever fast for forth from go high i.e. in into just later\n",
    "    low more off on open out over per pie raising start teeth that through\n",
    "    under unto up up-pp upon whole with you\n",
    "    \n",
    "* SYM: symbol\n",
    "\n",
    "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
    "    \n",
    "* TO: \"to\" as preposition or infinitive marker\n",
    "\n",
    "    to\n",
    "    \n",
    "* UH: interjection\n",
    "\n",
    "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
    "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
    "    man baby diddle hush sonuvabitch ...\n",
    "    \n",
    "* VB: verb, base form\n",
    "\n",
    "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
    "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
    "    boost brace break bring broil brush build ...\n",
    "    \n",
    "* VBD: verb, past tense\n",
    "\n",
    "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
    "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
    "    speculated wore appreciated contemplated ...\n",
    "    \n",
    "* VBG: verb, present participle or gerund\n",
    "\n",
    "    telegraphing stirring focusing angering judging stalling lactating\n",
    "    hankerin' alleging veering capping approaching traveling besieging\n",
    "    encrypting interrupting erasing wincing ...\n",
    "    \n",
    "* VBN: verb, past participle\n",
    "\n",
    "    multihulled dilapidated aerosolized chaired languished panelized used\n",
    "    experimented flourished imitated reunifed factored condensed sheared\n",
    "    unsettled primed dubbed desired ...\n",
    "    \n",
    "* VBP: verb, present tense, not 3rd person singular\n",
    "\n",
    "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
    "    appear tend stray glisten obtain comprise detest tease attract\n",
    "    emphasize mold postpone sever return wag ...\n",
    "    \n",
    "* VBZ: verb, present tense, 3rd person singular\n",
    "\n",
    "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
    "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
    "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
    "    \n",
    "* WDT: WH-determiner\n",
    "    that what whatever which whichever\n",
    "    \n",
    "* WP: WH-pronoun\n",
    "\n",
    "    that what whatever whatsoever which who whom whosoever\n",
    "    \n",
    "* WP$: WH-pronoun, possessive\n",
    "\n",
    "    whose\n",
    "    \n",
    "* WRB: Wh-adverb\n",
    "\n",
    "    how however whence whenever where whereby whereever wherein whereof why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"We hope you love your sleepycat mattress. Welcom to the sleepycat community.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
