{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling pipelines\n",
    "=================\n",
    "\n",
    "Thinking about modeling as a series of transformations is really helpful.\n",
    "Pipelines and functional transformations are the cleanest way to preprocess the data.\n",
    "It has its roots in Category theory from mathematics.\n",
    "\n",
    "Functional transformers are reusable and you can create many complicated things with them (think about Lego blocks).\n",
    "\n",
    "Assumptions\n",
    "-------------------\n",
    "\n",
    "1. We will be using scikit-learn interface to pipelines.\n",
    "2. We will use pandas dataframes as inputs to pipelines (useful).\n",
    "\n",
    "There are 2 types of building blocks of machine learning pipelines: transformers and estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers\n",
    "---------\n",
    "\n",
    "Blocks that have input and output and can be chained with other transformers.\n",
    "\n",
    "For example\n",
    "\n",
    "```\n",
    "Data -> [ Select variables ] -> [ Normalize ] -> [ Reduce dimensions ] -> Output\n",
    "```\n",
    "\n",
    "`[ Select variables ]` - transformer for selecting variables\n",
    "\n",
    "`[ Normalize ]` - normalization step\n",
    "\n",
    "`[ Reduce dimensions ]` - dimension reduction\n",
    "\n",
    "\n",
    "-------------------\n",
    "\n",
    "Because every transformer has the same type of data as input and output altogether they \n",
    "also form a transformer.\n",
    "\n",
    "```\n",
    "Input -> [ [ Select variables ] -> [ Normalize ] -> [ Reduce dimensions ] ] -> Output\n",
    "\n",
    "Input -> [               Data preprocessing transformation                ] -> Output\n",
    "```\n",
    "\n",
    "-------------------\n",
    "\n",
    "An example of transformer that does nothing\n",
    "\n",
    "```python\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LazyTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x\n",
    "```\n",
    "\n",
    "-------------------\n",
    "\n",
    "Notice that there are 2 methods:\n",
    "\n",
    "1. **fit** - learns the information about the data - it becomes a stateful transformer\n",
    "2. **transform** - applies the transformation \n",
    "\n",
    "There are 2 types of transformers:\n",
    "1. **stateful** - they learn something when calling fit method\n",
    "2. **stateless** - they don't learn anything\n",
    "\n",
    "**Why stateless transformers are useful?**\n",
    "\n",
    "Transformers that don't need historical data to learn can be used in a type of learning\n",
    "called `online learning`. This type of learning fits pipelines beacuse it is an algorithm\n",
    "that uses the stream of observations to learn.\n",
    "\n",
    "It doesn't keep the history so there would be no way to use stateful transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than modeling\n",
    "================\n",
    "\n",
    "Using pipelines is not limited to machine learning.\n",
    "It is as easy as defining a few rules to write modular and composable classes.\n",
    "\n",
    "Let's say you build a Data Engineering platform.\n",
    "\n",
    "Define a set of inputs (Let's say dataframes): A,B,C,D,E\n",
    "\n",
    "```python\n",
    "class Merge(Transformation):\n",
    "    input = (A,B)\n",
    "    output = (C)\n",
    "\n",
    "    def transform(A: DataFrame, B: DataFrame) -> DataFrame:\n",
    "        ...\n",
    "        return C\n",
    "        \n",
    "class ExtractUsefulFeatures(Transformation):\n",
    "    input = (C)\n",
    "    output = (D,E)\n",
    "    \n",
    "    def transform(C: DataFrame) -> (DataFrame, DataFrame):\n",
    "        ...\n",
    "        return (D,E)\n",
    "```\n",
    "\n",
    "Notice that every transform method accepts and outputs DataFrames. It is important to decouple IO operations\n",
    "\n",
    "\n",
    "```python\n",
    "def transform() -> DataFrame:\n",
    "    A = load_A()\n",
    "    B = load_B()\n",
    "    ...\n",
    "    return C\n",
    "```\n",
    "\n",
    "would be a mistake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn pipelines to the rescue\n",
    "-------------\n",
    "\n",
    "Fortunately scikit-learn provides a set of helpful functions to deal with pipelines.\n",
    "2 of them are the most important:\n",
    "\n",
    "1. `sklearn.pipeline.make_pipeline`\n",
    "\n",
    "2. `sklearn.pipeline.make_union`\n",
    "\n",
    "    Creates a union of transformers\n",
    "    \n",
    "    ```\n",
    "    \n",
    "             transformer 1\n",
    "           /               \\\n",
    "          /                 \\\n",
    "    input                     output\n",
    "          \\                 /    \n",
    "           \\               /\n",
    "             transformer 2\n",
    "             \n",
    "    ```\n",
    "             \n",
    "    It is useful when the dataset consists of several types of data that one must \n",
    "    deal with separately.\n",
    "\n",
    "\n",
    "Alternative way to define pipelines\n",
    "--------------\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "```\n",
    "\n",
    "It is useful to name the steps because sometimes we want to control the steps from outside - for example when searching for parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heterogenous data\n",
    "==========================\n",
    "\n",
    "Normally datasets are not matrices of numbers.\n",
    "In real life it will be a mix of:\n",
    "- categorical features\n",
    "- numerical features\n",
    "- dates\n",
    "- text data\n",
    "- with missing values / without missing values\n",
    "\n",
    "Still you must create 1 pipeline to process all these types of information.\n",
    "\n",
    "Possible transformations:\n",
    "- **categorical features**:\n",
    "    - one hot encoding - converting to binary values\n",
    "    - convert to numerical values - by using a hash of categorical variable\n",
    "    - target averaging - replace categorical feature with an average of the target\n",
    "    \n",
    "- **numerical features**:\n",
    "    - fill missing values\n",
    "    - create bins with ranges \n",
    "    - normalize, scale\n",
    "    \n",
    "- **text**\n",
    "    - use bag of words vectorization\n",
    "    - word2vec, sentence2vec\n",
    "\n",
    "- **dates**\n",
    "    - extract years, months, days, days of week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implimentation\n",
    "==========================\n",
    "\n",
    "\n",
    "Normally the data comes in various shapes and formats\n",
    "\n",
    "We need a way merge together sklearn and pandas dataframes in order to do something like this:\n",
    "\n",
    "```python\n",
    "pipeline = make_pipeline(\n",
    "     CleanData(),\n",
    "     make_union(\n",
    "         make_pipeline(\n",
    "             Selector('text_column'), \n",
    "             CountVectorizer()\n",
    "         ),\n",
    "         make_pipeline(\n",
    "             Selector('numerical_column_1', 'numerical_column_2'), \n",
    "             StandardScaler()\n",
    "         ),\n",
    "         make_pipeline(\n",
    "             Selector('categorical_column'), \n",
    "             OneHotEncoder()\n",
    "         ),\n",
    "      ),\n",
    "      model\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines are set up with the fit/transform/predict functionality, so you can fit a whole pipeline to the training data and transform to the test data, without having to do it individually for each thing you do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:32:54.778465Z",
     "start_time": "2019-12-15T06:32:54.242955Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:32:57.937349Z",
     "start_time": "2019-12-15T06:32:57.889031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hey no I ad a crap nite was borin without ya 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3704</th>\n",
       "      <td>ham</td>\n",
       "      <td>How is my boy? No sweet words left for me this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3025</th>\n",
       "      <td>ham</td>\n",
       "      <td>I love ya too but try and budget your money be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3899</th>\n",
       "      <td>ham</td>\n",
       "      <td>Otherwise had part time job na-tuition..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5352</th>\n",
       "      <td>ham</td>\n",
       "      <td>Good morning princess! Have a great day!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                            message\n",
       "976     ham  Hey no I ad a crap nite was borin without ya 2...\n",
       "3704    ham  How is my boy? No sweet words left for me this...\n",
       "3025    ham  I love ya too but try and budget your money be...\n",
       "3899    ham           Otherwise had part time job na-tuition..\n",
       "5352    ham           Good morning princess! Have a great day!"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd= pd.read_csv('data/smsspamcollection/SMSSpamCollection', sep='\\t', \n",
    "                names =['target','message'])\n",
    "sd.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:33:04.577637Z",
     "start_time": "2019-12-15T06:33:04.133866Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:33:04.656055Z",
     "start_time": "2019-12-15T06:33:04.648927Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sd['message'], \n",
    "                                                    sd['target'], \n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The usual way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:33:13.811070Z",
     "start_time": "2019-12-15T06:33:13.685125Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate the CountVectorizer method\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "# Fit the training data and then return the matrix\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "\n",
    "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
    "testing_data = count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:33:19.890318Z",
     "start_time": "2019-12-15T06:33:19.861872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Python pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:33:24.744919Z",
     "start_time": "2019-12-15T06:33:24.735357Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:34:07.383574Z",
     "start_time": "2019-12-15T06:34:07.380379Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe1=Pipeline([\n",
    "    ('countvec',CountVectorizer()),\n",
    "    ('classfier',MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:34:07.893042Z",
     "start_time": "2019-12-15T06:34:07.799154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('countvec',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('classfier',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:34:10.805623Z",
     "start_time": "2019-12-15T06:34:10.766603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.97122551e-01, 2.87744864e-03],\n",
       "       [9.99981651e-01, 1.83488846e-05],\n",
       "       [9.97926987e-01, 2.07301295e-03],\n",
       "       ...,\n",
       "       [9.99998910e-01, 1.09026171e-06],\n",
       "       [1.86697467e-10, 1.00000000e+00],\n",
       "       [9.99999996e-01, 3.98279868e-09]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save python objects to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:34:36.470864Z",
     "start_time": "2019-12-15T06:34:36.467977Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:34:41.499932Z",
     "start_time": "2019-12-15T06:34:41.212920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my_model_pipeline.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipe1,'my_model_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:34:51.314845Z",
     "start_time": "2019-12-15T06:34:51.311610Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:34:54.749546Z",
     "start_time": "2019-12-15T06:34:54.647299Z"
    }
   },
   "outputs": [],
   "source": [
    "mymodel=open('my_model_pipeline.pkl','rb')\n",
    "pipe=joblib.load(mymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:34:56.936443Z",
     "start_time": "2019-12-15T06:34:56.931784Z"
    }
   },
   "outputs": [],
   "source": [
    "my_msg=['I‘m going to try for 2 months ha ha only joking',\n",
    "        '''Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. \n",
    "        Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's''']\n",
    "my_df=pd.DataFrame({'message':my_msg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:34:57.410779Z",
     "start_time": "2019-12-15T06:34:57.402872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I‘m going to try for 2 months ha ha only joking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message\n",
       "0    I‘m going to try for 2 months ha ha only joking\n",
       "1  Free entry in 2 a wkly comp to win FA Cup fina..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:35:01.692329Z",
     "start_time": "2019-12-15T06:35:01.687009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99714387e-01, 2.85613245e-04],\n",
       "       [9.41423380e-22, 1.00000000e+00]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict_proba(my_df['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:35:33.305835Z",
     "start_time": "2019-12-15T06:35:33.301610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype='<U4')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
