{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Trees ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *tree* in computer science is a *data structure* (way of storing data) which looks like this:\n",
    "\n",
    "```\n",
    "         8\n",
    "       /   \\\n",
    "      5     7\n",
    "       \\   / \\\n",
    "        3 1   2\n",
    "             /\n",
    "            6\n",
    "```\n",
    "\n",
    "We'll be using them for *decision trees* (discussed below). You can think of those as a flow chart:\n",
    "\n",
    "![decision tree](images/decisiontree.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Decision trees](http://en.wikipedia.org/wiki/Decision_tree_learning) are one of the most popular and widely used algorithms in industry today. \n",
    "- Most classifiers (SVM, kNN, Naive Bayes) are great at giving you a (somewhat) accurate result, but are often black boxes in that it can be hard to interpret their parameters and thus *understand why* a certain instance was assigned a specific label. Regressions provide a middle ground in that it is possible to assign relationships between features and targets but it is often to assign a causal relationship amongst the inputs \n",
    "- Decision trees are unique among any standard model in that they are both flexible and accurate while also being easily interpreted.\n",
    "\n",
    "Also, refer: https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Tradeoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Decision Trees\n",
    "\n",
    "* Easily interpretable\n",
    "* Handles missing values and outliers\n",
    "* [non-parametric](http://en.wikipedia.org/wiki/Non-parametric_statistics#Non-parametric_models)/[non-linear](http://www.yaksis.com/static/img/02/cows_and_wolves.png)/model complex phenomenom\n",
    "* Computationally _cheap_ to ___predict___\n",
    "* Can handle irrelevant features\n",
    "* Mixed data (nominal and continuous)\n",
    "\n",
    "#### Why Not Decision Trees\n",
    "\n",
    "* Computationally _expensive_ to ___train___\n",
    "* Greedy algorithm (local optima)\n",
    "* Very easy to overfit\n",
    "\n",
    "#### Naive Bayes vs Decision Tree\n",
    "- Naive Bayes includes all predictors and assumes independence between predictors. \n",
    "- Decision Tree also includes all predictors but with dependence assumptions between predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- root node\n",
    "- internal node/decision node\n",
    "- branch\n",
    "- leaf node\n",
    "\n",
    "![](images/tree_ter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the below dataset. It shows whether or not a person would play tennis based on some conditions:\n",
    "\n",
    "<center><img src=\"images/data.png\" width=\"600\"/></center>\n",
    "\n",
    "\n",
    "The decision tree for the above problem can look something like this:\n",
    "\n",
    "![](images/golftree.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3  Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For designing the decision tree we have to first decide on the root node. In order to decide the root node, we have to chose the feature which best predicts our target class. This is done using \"Entropy\" and \"Information Gain\". The *gini impurity* is another alternative, which we'll discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, we need to discuss *entropy*. The entropy of a set is a measure of the amount of disorder. Intuitively, if a set has all the same labels, that'll have low entropy and if it has a mix of labels, that's high entropy. We would like to create splits that minimize the entropy in each size. If our splits do a good job splitting along the boundary between classes, they have more predictive power.\n",
    "\n",
    "- The intuition of entropy is more important than the actual function, which follows.\n",
    "\n",
    "![entropy](images/entropy.png)\n",
    "\n",
    "Here, P(c) is the percent of the group that belongs to a given class.\n",
    "\n",
    "If you have a collection of datapoints, the entropy will be large when they are evenly distributed across the classes and small when they are mostly the same class. Here's a graph to demonstrate what entropy looks like:\n",
    "\n",
    "![entropy](images/entropy_graph_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we would like splits that minimize entropy. We use *information gain* to determine the best split:\n",
    "\n",
    "![information gain](images/gain.png)\n",
    "\n",
    "Here, S is the original set and D is the splitting of the set (a partition). Each V is a subset of S. All of the V's are disjoint and make up S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding ID3 algorithm with Golf Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First calculate Entropy for target variable:\n",
    "![Step 1](images/step1.png)\n",
    "\n",
    "#### Next Calculate Conditional Entropy for all the features\n",
    "![Step 2](images/step2.png)\n",
    "\n",
    "#### Next Calculate Entropy gain of target variable wrt all features\n",
    "![Step 3](images/step3.png)\n",
    "\n",
    "#### recursively run ID3 algorithm on the non-leaf branches, until all data is classified.\n",
    "![Step 4](images/step4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use Decision Trees for regression! Instead of taking the majority vote at each leaf node, if you're trying to predict a continuous value, you can average the values. You can also use a combination of decision trees and linear regression on the leaf nodes (called *model trees*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterizing the Output of Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundaries of classification trees is one of the easiest things to characterize and understand in machine learning. We can actually draw out the result of the decision process in terms of a set of subdivisions of the output space:\n",
    "\n",
    "<center><img src=\"images/DecisionTree_2.gif\" width=\"700\"/></center>\n",
    "\n",
    "Each split of the tree can be characterized as a simple line dividing all points with the selected attribute along that axis to points above and below the selection. This image is described as the **partitioning** of the input space, where the boundaries of each domain are determined based on the branch points of the tree. The partitioning of a correctly-built classification tree amounts to a continuous stepped line showing the boundary between the two classes, with each corner on the step describing the boundary of that tree split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like many - if not most - standard **classifiers**, trees and by extension, forests, can be converted into **regressors** and thus used for continuous predictions. In the case of single trees, the tree parses the inputs into large domains, and produces a single output (based on the training data) as an output of that domain (where the domain is defined by the splits).\n",
    "\n",
    "In the below figures we see the construction of trees for predicting car price. Below is a standard tree regressed against horsepower and wheelbase: \n",
    "\n",
    "![tree1](images/Wheelbase_tree_3.png)\n",
    "\n",
    "Below is the partitioning of the regression:\n",
    "\n",
    "![partition](images/Tree_Parsing_1.png)\n",
    "\n",
    "\n",
    "It's worth noting that if multiple predictor variables (features) are equally good at determining the outcome, the split chosen at that branch level is determined by chance. This is the essential concept behind the notion of *feature importance*. Here is the same tree as above, regressed against weight instead of wheelbase at the second level:\n",
    "\n",
    "![tree1](images/Weight_tree_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, there are several decisions to be made when building a decision tree:\n",
    "\n",
    "* Whether to split categorial features fully or binary\n",
    "* If and how to do pruning\n",
    "\n",
    "There is some terminology to the different variants. Some of them are proprietary algorithms so we don't yet know all the parts.\n",
    "\n",
    "#### ID3\n",
    "Short for Iterative Dichotomiser 3, the original Decision Tree algorithm developed by Ross Quinlan (who's responsible for a lot of proprietary decision tree algorithms) in the 1980's.\n",
    "\n",
    "* designed for only categorial features\n",
    "* splits categorical features completely\n",
    "* uses entropy and information gain to pick the best split\n",
    "\n",
    "#### CART\n",
    "Short for Classification and Regression Tree was invented about the same time as ID3 by Breiman, Friedman, Olshen and Stone. The CART algorithm has the following properties:\n",
    "\n",
    "* handles both categorial and continuous data\n",
    "* always uses binary splits\n",
    "* uses gini impurity to pick the best split\n",
    "\n",
    "Algorithms will be called CART even if they don't follow all of the specifications of the original algorithm.\n",
    "\n",
    "#### Gini impurity\n",
    "\n",
    "The *Gini impurity* is another way of measuring which split is the best. It's a measure of this probability:\n",
    "\n",
    "* Take a random element from the set\n",
    "* Label it randomly according to the distribution of labels in the set\n",
    "* What is the probability that it is labeled incorrectly?\n",
    "\n",
    "This is the gini impurity:\n",
    "\n",
    "![gini impurity](images/gini.png)\n",
    "\n",
    "#### C4.5\n",
    "This is Quinlan's first improvement on the ID3 algorithm. The main improvements are:\n",
    "\n",
    "* handles continuous data\n",
    "* implements pruning to reduce overfitting\n",
    "\n",
    "There is now a **C5.0** which is supposedly better, but is propietary so we don't have access to the specifics of the improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting & Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are prone to overfitting. If we have a lot of features and they all get used in building our tree, we will build a tree that perfectly represents our training data but is not general. A way to relax this is *pruning*. The idea is that we may not want to continue building the tree until all the leaves are pure (have only datapoints of one class). There are two main ways of pruning: *prepruning* and *postpruning*.\n",
    "\n",
    "### Prepruning\n",
    "*Prepruning* is making the decision tree algorithm stop early. Here are a few ways that we preprune:\n",
    "\n",
    "* leaf size: Stop when the number of data points for a leaf gets below a threshold\n",
    "* depth: Stop when the depth of the tree (distance from root to leaf) reaches a threshold\n",
    "* mostly the same: Stop when some percent of the data points are the same (rather than all the same)\n",
    "* error threshold: Stop when the error reduction (information gain) isn't improved significantly.\n",
    "\n",
    "### Postpruning\n",
    "As the name implies, *postpruning* involves building the tree first and then choosing to cut off some of the leaves (shorten some of the branches, the tree analogy really works well here).\n",
    "\n",
    "Practically, the second approach of post-pruning overfit trees is more successful because it is not easy to precisely estimate when to stop growing the tree. \n",
    "\n",
    "Here's the psuedocode:\n",
    "\n",
    "```\n",
    "function Prune:\n",
    "    if either left or right is not a leaf:\n",
    "        call Prune on that split\n",
    "    if both left and right are leaf nodes:\n",
    "        calculate error associated with merging two nodes\n",
    "        calculate error associated without merging two nodes\n",
    "        if merging results in lower error:\n",
    "            merge the leaf nodes\n",
    "```\n",
    "\n",
    "The important step of tree pruning is to define a criterion be used to determine the correct final tree size using one of the following methods:\t\t\n",
    "- Use a distinct dataset from the training set (called validation set), to evaluate the effect of post-pruning nodes from the tree.\n",
    "- Build the tree by using the training set, then apply a statistical test to estimate whether pruning or expanding a particular node is likely to produce an improvement beyond the training set.\n",
    "    - Error estimation\n",
    "    - Significance testing (e.g., Chi-square test)\n",
    "    \n",
    "## Super Attributes\n",
    "\n",
    "The information gain equation, $G(T,X)$ is biased toward attributes that have a large number of values over attributes that have a smaller number of values. These ‘Super Attributes’ will easily be selected as the root, resulted in a broad tree that classifies perfectly but performs poorly on unseen instances. We can penalize attributes with large numbers of values by using an alternative method for attribute selection, referred to as Gain Ratio.\n",
    "\n",
    "![Step 4](images/super_attr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters in Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn:\n",
    "\n",
    "```\n",
    "class sklearn.tree.DecisionTreeClassifier(\n",
    "    criterion=’gini’, \n",
    "    splitter=’best’, \n",
    "    max_depth=None, \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_features=None, \n",
    "    random_state=None, \n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    min_impurity_split=None, \n",
    "    class_weight=None, \n",
    "    presort=False)\n",
    "```\n",
    "\n",
    "read more [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- max_depth: The max depth of the tree where we will stop splitting the nodes. This is similar to controlling the maximum number of layers in a deep neural network. Lower will make your model faster but not as accurate; higher can give you accuracy but risks overfitting and may be slow.\n",
    "\n",
    "- min_samples_split: The minimum number of samples required to split a node. We discussed this aspect of decision trees above and how setting it to a higher value would help mitigate overfitting.\n",
    "\n",
    "- max_features: The number of features to consider when looking for the best split. Higher means potentially better results with the tradeoff of training taking longer.\n",
    "\n",
    "- min_impurity_split: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold. This can be used to tradeoff combating overfitting (high value, small tree) vs high accuracy (low value, big tree).\n",
    "\n",
    "- presort: Whether to presort the data to speed up the finding of best splits in fitting. If we sort our data on each feature beforehand, our training algorithm will have a much easier time finding good values to split on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In practice\n",
    "\n",
    "In practice:\n",
    "\n",
    "* We always implement pruning to avoid overfitting\n",
    "* Either gini or information gain is acceptable\n",
    "* Sometimes fully splitting categorial features is preferred, but generally we err on the side of binary splits (simpler and doesn't run into issues when a feature has many potential values)\n",
    "\n",
    "In `sklearn` ([documentation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier))\n",
    "\n",
    "* Pruning with `max_depth`, `min_samples_split`, `min_samples_leaf` or `max_leaf_nodes`\n",
    "* gini is default, but you can also choose entropy\n",
    "* does binary splits (you would need to binarize categorial features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://www.saedsayad.com/decision_tree_overfitting.htm"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
