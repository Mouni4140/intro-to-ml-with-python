{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ensemble Methods* combine multiple machine learning algorithms to obtain better predictive performance. The idea is simple: run multiple models on the data and use their predictions to make a prediction that is better than any of the models could do alone.\n",
    "\n",
    "Ensemble methods can:\n",
    "- decrease variance using bagging approach, \n",
    "- decrease bias using a boosting approach, or \n",
    "- improve predictions using stacking approach.\n",
    "\n",
    "<center><img src=\"images/bagging_vs_boosting.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main idea behind the structure of a stacked generalization (or stacking) is to use one or more first level models, make predictions using these models and then use these predictions as features to fit one or more second level models on top.\n",
    "\n",
    "- Any model can be used as 1st level model or 2nd level model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Split the train set into n parts (say 10).\n",
    "<center><img src=\"images/stack1.png\" width=\"300\"/></center>\n",
    "\n",
    "Step 2: A base model (suppose a decision tree) is fitted on 9 parts and predictions are made for the 10th part. This is done for each part of the train set.\n",
    "<center><img src=\"images/stack2.png\" width=\"300\"/></center>\n",
    "\n",
    "Step 3: The base model (in this case, decision tree) is then fitted on the whole train dataset.\n",
    "\n",
    "Step 4: Using this model, predictions are made on the test set.\n",
    "<center><img src=\"images/stack3.png\" width=\"300\"/></center>\n",
    "\n",
    "Step 5: Steps 2 to 4 are repeated for another base model (say knn) resulting in another set of predictions for the train set and test set.\n",
    "<center><img src=\"images/stack4.png\" width=\"300\"/></center>\n",
    "\n",
    "Step 6: The predictions from the train set are used as features to build a new model.\n",
    "<center><img src=\"images/stack5.png\" width=\"300\"/></center>\n",
    "\n",
    "Step 7: This model is used to make final predictions on the test prediction set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variation A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/vara_1.png\" width=\"500\"/></center>\n",
    "\n",
    "<center><img src=\"images/vara_2.png\" width=\"500\"/></center>\n",
    "\n",
    "<center><img src=\"images/vara_3.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variation B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/varb_1.png\" width=\"600\"/></center>\n",
    "\n",
    "<center><img src=\"images/varb_2.png\" width=\"600\"/></center>\n",
    "\n",
    "<center><img src=\"images/varb_3.png\" width=\"600\"/></center>\n",
    "\n",
    "<center><img src=\"images/varb_4.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging (developed by late Berkeley prof Leo Breiman), also known as **_bootstrap aggregating_**, is for running multiple models in parallel (the models don't use each other's results in order to predict). \n",
    "- Each model gets a vote on the final prediction. \n",
    "- The majority vote (again determined by a user-set parameter) is what is reported from the bagged model. \n",
    "\n",
    "Bagging is intended to address the problem of sample bias by enabling the model to capture bias within the training sample itself.\n",
    "\n",
    "\n",
    "<center><img src=\"images/BaggingCropped_2.png\" width=\"700\"/></center>\n",
    "\n",
    "\n",
    "For classification problems (predicting a categorical value), we choose the label with the most votes.\n",
    "\n",
    "For regression problems (predicting a continuous value), we average the values given by all the models.\n",
    "\n",
    "You can bag with any single algorithm or collection of algorithms, giving them each a vote to the final prediction. This means you can leverage the voting power of several different kinds of models, assuming that they have similar output space (what if they do not?). Scikits have a simple Bagging API implemented so that you can bag any [regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html) or [classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html). [This example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html) is a useful one to study.\n",
    "\n",
    "In the case of decision trees, we extend this cognate to Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model. \n",
    "\n",
    "The boosting algorithm combines a number of weak learners to form a strong learner. The individual models would not perform well on the entire dataset, but they work well for some part of the dataset. Thus, each model actually boosts the performance of the ensemble.\n",
    "\n",
    "Examples of Boosting algorithms:\n",
    "\n",
    "- AdaBoost\n",
    "- GBM\n",
    "- XGBM\n",
    "- Light GBM\n",
    "- CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Random Forests by Leo Breiman and Adele Cutler](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)\n",
    "* [sklearn ensembles](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "* [Applied Data Science](http://columbia-applied-data-science.github.io/appdatasci.pdf) (Section 9.4.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
