{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Boosting\n",
    "\n",
    "## Boosting in Trees\n",
    "\n",
    "There are two types of boosting used in trees in common practice todayThe first type of boosting is the use of one model to precondition the inputs of another model. This is the most common jargon usage of this word in data science. If you hear fellow data scientists say \"boosting\", they are often referring to boosted trees. \n",
    "\n",
    "### Adding bias into the training process\n",
    "\n",
    "In the boosted tree algorithms, a ensemble of trees is \"grown\" over successive generations of trees, by making each successor tree an expert on attacking the weaknesses of the other. \n",
    "\n",
    "What results is a forest of trees designed to defeat all weaknesses of the training set. Members of the ensemble are weighted by the ratio of errors not covered by the previous members.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "\n",
    "- AdaBoost focuses on providing each tree a description of the mistakes of the last tree (through the \"loss\" function). \n",
    "- Each tree is given the opportunity to **Ada**pt itself to the weaknesses of previous trees. \n",
    "- Finally the end classifier is something like a random forest, where each of the trees votes, weighted inversely to its own total error.\n",
    "\n",
    "## Adaboost should meet two conditions:\n",
    "\n",
    "- The classifier should be trained interactively on various **weighted training examples**.\n",
    "- In each iteration, it tries to provide an excellent fit for these examples by minimizing training error.\n",
    "\n",
    "## How does the AdaBoost algorithm work?\n",
    "\n",
    "It works in the following steps:\n",
    "\n",
    "- Initially, Adaboost selects a training subset randomly.\n",
    "- It iteratively trains the AdaBoost machine learning model by selecting the training set based on the accurate prediction of the last training.\n",
    "- It assigns the higher weight to wrong classified observations so that in the next iteration these observations will get the high probability for classification.\n",
    "- Also, It assigns the weight to the trained classifier in each iteration according to the accuracy of the classifier. The more accurate classifier will get high weight.\n",
    "\n",
    "<center><img src=\"images/adaboost.png\" width=\"500\"/></center>\n",
    "\n",
    "![adaboost_scheme](./images/AdaBoost_scheme.png)\n",
    "\n",
    "Below is shown a plot of the AdaBoost partition:\n",
    "\n",
    "![adaboost_part](./images/AdaBoost_part.png)\n",
    "\n",
    "Successive trees end up covering the limitations of previous trees. The size of the points reflect their weighting at that point. The misclassified points are *individually* penalized, just as misclassified points are penalized in SVMs. \n",
    "\n",
    "The next tree in the family may or may not correctly classify these points (it is still just a simple decision tree), but if it does, then the total cost function comes down, and it is more favorable to keep the tree. If it does not, the weights continue to increase until a tree comes along that correctly classifies these points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example:\n",
    "\n",
    "<center><img src=\"images/ab1.png\" width=\"200\"/></center>\n",
    "\n",
    "<center><img src=\"images/ab2.png\" width=\"600\"/></center>\n",
    "\n",
    "<center><img src=\"images/ab3.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "- **base_estimator**: It is a weak learner used to train the model. It uses DecisionTreeClassifier as default weak learner for training purpose. You can also specify different machine learning algorithms.\n",
    "\n",
    "- **n_estimators**: Number of weak learners to train iteratively.\n",
    "\n",
    "- **learning_rate**: It contributes to the weights of weak learners. It uses 1 as a default value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the Weights are Used\n",
    "\n",
    "The weights $w_{i}$ are used in the impurity estimator during the split, making it progressively more unfavorable when we calculate the information gain if they are misclassified (fall inside a leaf belonging to another class - the weights increase the equivalent counts of a class belonging to that point). This results in a powerful increase in impurity if this point is misclassified.\n",
    "\n",
    "\n",
    "\n",
    "## The AdaBoost Algorithm\n",
    "\n",
    "    Fit:\n",
    "    \n",
    "        Initialize weights W to be 1/N\n",
    "        for m in range(1,M): (M determined by user)\n",
    "            fit a standard decision tree G(X,w)\n",
    "            compute total error err <- sum(W*misclassified points)/sum(W)\n",
    "            compute alpha_i <- log((1-err)/err)\n",
    "            change weights by an exponent if misclassified w_i <- \n",
    "            w_i * exp(alpha_i)\n",
    "   \n",
    "    Predict:\n",
    "     \n",
    "        for m in range(1, M):\n",
    "            sum += alpha_m * G_m(X)\n",
    "        return sign(sum)\n",
    "\n",
    "\n",
    "### Some Details:\n",
    "\n",
    "Thus the most specific trees (at the end of the boost chain) are often the most dilute (reflect the fewest points), but they are weighted (by error score) much more than the earliest. Many of the later trees may specify for only a tiny number of points. The mixture of different trees can manage outliers and noise much better than otherwise.\n",
    "\n",
    "Adaboost is often affected profoundly by the choice of prepruning parameters. This can be thought of in terms of the tree's selection of partition boundary. If the boundaries for an individual tree become complex, we are eschewing the algorithm's ability to weight the votes of later trees to manage small numbers of specific points in favor of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import datasets\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,), (array([0, 1, 2]), array([50, 50, 50])))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape, np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adaboost classifer object\n",
    "abc = AdaBoostClassifier(n_estimators=50,\n",
    "                         learning_rate=1)\n",
    "# Train Adaboost Classifer\n",
    "model = abc.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "# Import Support Vector Classifier\n",
    "from sklearn.svm import SVC\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "svc=SVC(probability=True, kernel='linear')\n",
    "\n",
    "# Create adaboost classifer object\n",
    "abc =AdaBoostClassifier(n_estimators=50, base_estimator=svc, \n",
    "                        learning_rate=1)\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "model = abc.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "http://www.inf.fu-berlin.de/inst/ag-ki/adaboost4.pdf - short paper with clean derivation  \n",
    "https://en.wikipedia.org/wiki/AdaBoost  \n",
    "http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf - Includes description of importance sampling\n",
    "http://statweb.stanford.edu/~jhf/ftp/boost.pdf - Friedman's analysis of adaboost  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
